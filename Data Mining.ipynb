{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9e3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from github import Github, RateLimitExceededException\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import time\n",
    "import re\n",
    "\n",
    "ACCESS_TOKEN = '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'\n",
    "client = Github(ACCESS_TOKEN, per_page=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649a5f9",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkorange\">Availability Bias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06b762",
   "metadata": {},
   "source": [
    "#### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f3f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Repo_List = list(pd.read_csv('consideredProjects.csv')['repo_name'])\n",
    "\n",
    "# Minimum Number of PR comments to be considered \n",
    "min_pr_comnt = 5\n",
    "\n",
    "# Initializing format of Availability dataset\n",
    "try: \n",
    "    df = pd.read_csv('availability_data_set.csv', index_col = [0])\n",
    "    avail_data_dic = df.to_dict('list')\n",
    "    \n",
    "    json_file = open('avail_latest_logger.json', 'r')\n",
    "    avail_log_tracker = json.load(json_file)\n",
    "    \n",
    "except:\n",
    "    avail_data_dic = {'Repo':[], 'pr_id':[] , 'pr_url':[], 'pr_author':[], 'reviewer':[], 'review_comment_body':[] \n",
    "                     ,'review_comment_URL':[],'review_comment_id':[], 'in_reply_to_id':[], 'created_at':[], 'position':[]\n",
    "                     ,'original_position':[], 'diff_hunk':[], 'commit_id':[], 'path':[]}\n",
    "    \n",
    "    avail_log_tracker = {'latest_repo': None, 'latest_pr': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae29c7",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb447e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_pr_review_comments(pr, repo, avail_data_dic):\n",
    "    for review_comment in pr.get_review_comments():\n",
    "        try:\n",
    "            # Excluding review comments by bots or author of the PR\n",
    "            if (pr.user.login != review_comment.user.login) and ('bot' not in review_comment.user.login):\n",
    "                avail_data_dic['Repo'].append(repo.full_name)\n",
    "                avail_data_dic['pr_id'].append(pr.id)\n",
    "                avail_data_dic['pr_url'].append(pr.url)\n",
    "                avail_data_dic['pr_author'].append(pr.user.login)\n",
    "                avail_data_dic['review_comment_body'].append(review_comment.body)\n",
    "                avail_data_dic['reviewer'].append(review_comment.user.login)\n",
    "                avail_data_dic['review_comment_URL'].append(review_comment.html_url)\n",
    "                avail_data_dic['review_comment_id'].append(review_comment.id)\n",
    "                avail_data_dic['in_reply_to_id'].append(review_comment.in_reply_to_id)\n",
    "                avail_data_dic['position'].append(review_comment.position)\n",
    "                avail_data_dic['created_at'].append(review_comment.created_at)\n",
    "                avail_data_dic['diff_hunk'].append(review_comment.diff_hunk)\n",
    "                avail_data_dic['original_position'].append(review_comment.original_position)\n",
    "                avail_data_dic['commit_id'].append(review_comment.commit_id)\n",
    "                avail_data_dic['path'].append(review_comment.path)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return avail_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6f714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def availability_data_set(Repo_List, avail_data_dic, avail_log_tracker):\n",
    "    if avail_log_tracker['latest_repo'] != None:\n",
    "        # We had some runs and we should continue not start from beginning\n",
    "        start = Repo_List.index(avail_log_tracker['latest_repo'])\n",
    "        \n",
    "    else:\n",
    "        start = 0\n",
    "        \n",
    "    for REPO in Repo_List[start:]:\n",
    "        # Keep track of where we are in terms of repo\n",
    "        avail_log_tracker['latest_repo'] = REPO\n",
    "        \n",
    "        print(\"Extracting PR for Repo: {}\".format(REPO))\n",
    "        repo = client.get_repo(REPO)\n",
    "        closed_prs = list(repo.get_pulls(state=\"closed\", sort=\"popularity\", direction=\"desc\"))\n",
    "        \n",
    "        pr_counter = avail_log_tracker['latest_pr']\n",
    "        \n",
    "        for pr in closed_prs[pr_counter:]:\n",
    "            # Keep track of where we are in terms of pr_id\n",
    "            pr_counter += 1\n",
    "            avail_log_tracker['latest_pr'] = pr_counter\n",
    "            \n",
    "            search_rate_limit = client.get_rate_limit().core\n",
    "            if pr_counter%100 == 0:\n",
    "                print(\"###############\", \" Extracting PR #{} \".format(pr_counter), \"###############\")\n",
    "                print(f\"With this token we still have {search_rate_limit.remaining} requests. \"\n",
    "                      f\"The quota will be reset at {search_rate_limit.reset}\")\n",
    "                        \n",
    "            # Ensuring that only PR with more than min# of comments will be passed\n",
    "            if pr.get_issue_comments().totalCount >= min_pr_comnt:\n",
    "            \n",
    "                try:\n",
    "                    \n",
    "                    avail_data_dic = import_pr_review_comments(pr, repo, avail_data_dic)      \n",
    "\n",
    "                    if search_rate_limit.remaining < 50:\n",
    "                        reset_timestamp = calendar.timegm(search_rate_limit.reset.timetuple())\n",
    "                        sleep_time = reset_timestamp - calendar.timegm(time.gmtime()) + 10\n",
    "                        print('Going to sleep untill {}'.format(search_rate_limit.reset))\n",
    "                        time.sleep(sleep_time)\n",
    "\n",
    "                except RateLimitExceededException:\n",
    "                    search_rate_limit = client.get_rate_limit().core\n",
    "                    print('Going to sleep untill {}'.format(search_rate_limit.reset))\n",
    "                    reset_timestamp = calendar.timegm(search_rate_limit.reset.timetuple())\n",
    "                    sleep_time = reset_timestamp - calendar.timegm(time.gmtime()) + 10\n",
    "                    time.sleep(sleep_time)\n",
    "        \n",
    "        # Reseting pr counting to 0 for the next REPO\n",
    "        avail_log_tracker['latest_pr'] = 0\n",
    "\n",
    "    return avail_data_dic, avail_log_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6456b729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting PR for Repo: eclipse-openj9/openj9\n",
      "###############  Extracting PR #900  ###############\n",
      "With this token we still have 4086 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1000  ###############\n",
      "With this token we still have 3548 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1100  ###############\n",
      "With this token we still have 2983 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1200  ###############\n",
      "With this token we still have 2333 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1300  ###############\n",
      "With this token we still have 1715 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1400  ###############\n",
      "With this token we still have 1215 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1500  ###############\n",
      "With this token we still have 732 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1600  ###############\n",
      "With this token we still have 192 requests. The quota will be reset at 2022-11-15 12:05:10\n",
      "Going to sleep untill 2022-11-15 12:05:10\n",
      "###############  Extracting PR #1700  ###############\n",
      "With this token we still have 4674 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #1800  ###############\n",
      "With this token we still have 4149 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #1900  ###############\n",
      "With this token we still have 3640 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2000  ###############\n",
      "With this token we still have 3184 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2100  ###############\n",
      "With this token we still have 2767 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2200  ###############\n",
      "With this token we still have 2299 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2300  ###############\n",
      "With this token we still have 1929 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2400  ###############\n",
      "With this token we still have 1613 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2500  ###############\n",
      "With this token we still have 1237 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2600  ###############\n",
      "With this token we still have 863 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2700  ###############\n",
      "With this token we still have 516 requests. The quota will be reset at 2022-11-15 13:05:26\n",
      "Going to sleep untill 2022-11-15 13:05:26\n",
      "###############  Extracting PR #2800  ###############\n",
      "With this token we still have 4936 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #2900  ###############\n",
      "With this token we still have 4573 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3000  ###############\n",
      "With this token we still have 4141 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3100  ###############\n",
      "With this token we still have 3800 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3200  ###############\n",
      "With this token we still have 3290 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3300  ###############\n",
      "With this token we still have 2927 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3400  ###############\n",
      "With this token we still have 2714 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3500  ###############\n",
      "With this token we still have 2614 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3600  ###############\n",
      "With this token we still have 2514 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3700  ###############\n",
      "With this token we still have 2414 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3800  ###############\n",
      "With this token we still have 2314 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #3900  ###############\n",
      "With this token we still have 2214 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4000  ###############\n",
      "With this token we still have 2114 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4100  ###############\n",
      "With this token we still have 2014 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4200  ###############\n",
      "With this token we still have 1914 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4300  ###############\n",
      "With this token we still have 1814 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4400  ###############\n",
      "With this token we still have 1714 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4500  ###############\n",
      "With this token we still have 1614 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4600  ###############\n",
      "With this token we still have 1514 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4700  ###############\n",
      "With this token we still have 1414 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4800  ###############\n",
      "With this token we still have 1314 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #4900  ###############\n",
      "With this token we still have 1214 requests. The quota will be reset at 2022-11-15 14:05:40\n",
      "###############  Extracting PR #5000  ###############\n",
      "With this token we still have 4977 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5100  ###############\n",
      "With this token we still have 4877 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5200  ###############\n",
      "With this token we still have 4777 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5300  ###############\n",
      "With this token we still have 4677 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5400  ###############\n",
      "With this token we still have 4577 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5500  ###############\n",
      "With this token we still have 4477 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5600  ###############\n",
      "With this token we still have 4377 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5700  ###############\n",
      "With this token we still have 4277 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5800  ###############\n",
      "With this token we still have 4177 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #5900  ###############\n",
      "With this token we still have 4077 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6000  ###############\n",
      "With this token we still have 3977 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6100  ###############\n",
      "With this token we still have 3877 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6200  ###############\n",
      "With this token we still have 3777 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6300  ###############\n",
      "With this token we still have 3677 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6400  ###############\n",
      "With this token we still have 3577 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6500  ###############\n",
      "With this token we still have 3477 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6600  ###############\n",
      "With this token we still have 3377 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6700  ###############\n",
      "With this token we still have 3277 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6800  ###############\n",
      "With this token we still have 3177 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #6900  ###############\n",
      "With this token we still have 3077 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7000  ###############\n",
      "With this token we still have 2977 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7100  ###############\n",
      "With this token we still have 2877 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7200  ###############\n",
      "With this token we still have 2777 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7300  ###############\n",
      "With this token we still have 2677 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7400  ###############\n",
      "With this token we still have 2577 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7500  ###############\n",
      "With this token we still have 2477 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7600  ###############\n",
      "With this token we still have 2377 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7700  ###############\n",
      "With this token we still have 2277 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7800  ###############\n",
      "With this token we still have 2177 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #7900  ###############\n",
      "With this token we still have 2077 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8000  ###############\n",
      "With this token we still have 1977 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8100  ###############\n",
      "With this token we still have 1877 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8200  ###############\n",
      "With this token we still have 1777 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8300  ###############\n",
      "With this token we still have 1677 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8400  ###############\n",
      "With this token we still have 1577 requests. The quota will be reset at 2022-11-15 15:05:41\n",
      "###############  Extracting PR #8500  ###############\n",
      "With this token we still have 4966 requests. The quota will be reset at 2022-11-15 16:05:42\n",
      "###############  Extracting PR #8600  ###############\n",
      "With this token we still have 4866 requests. The quota will be reset at 2022-11-15 16:05:42\n",
      "###############  Extracting PR #8700  ###############\n",
      "With this token we still have 4766 requests. The quota will be reset at 2022-11-15 16:05:42\n",
      "###############  Extracting PR #8800  ###############\n",
      "With this token we still have 4666 requests. The quota will be reset at 2022-11-15 16:05:42\n",
      "###############  Extracting PR #8900  ###############\n",
      "With this token we still have 4566 requests. The quota will be reset at 2022-11-15 16:05:42\n",
      "###############  Extracting PR #9000  ###############\n",
      "With this token we still have 4466 requests. The quota will be reset at 2022-11-15 16:05:42\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    avail_data_dic, avail_log_tracker = availability_data_set(Repo_List, avail_data_dic, avail_log_tracker)\n",
    "    \n",
    "    # Keep where we have stopped\n",
    "    tf = open(\"avail_latest_logger.json\", \"w\")\n",
    "    json.dump(avail_log_tracker, tf)\n",
    "    tf.close()\n",
    "\n",
    "    df = pd.DataFrame(avail_data_dic)\n",
    "    df.to_csv('availability_data_set.csv')\n",
    "    \n",
    "except:\n",
    "    # Keep where we have stopped\n",
    "    tf = open(\"avail_latest_logger.json\", \"w\")\n",
    "    json.dump(avail_log_tracker, tf)\n",
    "    tf.close()\n",
    "    \n",
    "    df = pd.DataFrame(avail_data_dic)\n",
    "    df.to_csv('availability_data_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0b832",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkorange\">Anchoring Bias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed360387",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080fe476",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers = list(pd.read_csv('Reviewers.csv', index_col = [0])['reviewer'])\n",
    "\n",
    "# extract text start with '<a title=\"' and end with </a>. And extract again text that start with 'href=\"/' \n",
    "# and end with '\">' to achive the final URLs.\n",
    "query1 = '<a title=\"(.+?)</a>'\n",
    "query2 = 'href=\"/(.+?)\">'\n",
    "\n",
    "# Initializing format of Anchoring dataset\n",
    "try:\n",
    "    df = pd.read_csv('anchoring_data_set.csv', index_col = [0])\n",
    "    anchor_data_dic = df.to_dict('list')\n",
    "    \n",
    "    json_file = open('anchor_latest_logger.json', 'r')\n",
    "    anchor_log_tracker = json.load(json_file)\n",
    "    \n",
    "except:\n",
    "    anchor_data_dic = {'reviewer':[], 'Repo':[], 'pr_id':[], 'pr_num':[] , 'review_comment_body':[] \n",
    "                      ,'review_comment_URL':[],'review_comment_id':[], 'created_at':[], 'position':[]\n",
    "                      ,'original_position':[], 'diff_hunk':[], 'commit_id':[], 'path':[]}\n",
    "    \n",
    "    anchor_log_tracker = {'latest_reviewer': None, 'latest_link': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc05273",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a299b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_reviewer_review_comments(reviewer, pr, repo, anchor_data_dic):\n",
    "    # reviewer should'nt be the creator of the PR\n",
    "    if pr.user.login != reviewer:\n",
    "        for review_comment in pr.get_review_comments():\n",
    "            try:\n",
    "                # Save only reviews of the specific reviewer\n",
    "                if review_comment.user.login == reviewer:\n",
    "                    anchor_data_dic['Repo'].append(repo.full_name)\n",
    "                    anchor_data_dic['pr_id'].append(pr.id)\n",
    "                    anchor_data_dic['pr_num'].append(pr.number)\n",
    "                    anchor_data_dic['review_comment_body'].append(review_comment.body)\n",
    "                    anchor_data_dic['reviewer'].append(reviewer)\n",
    "                    anchor_data_dic['review_comment_URL'].append(review_comment.html_url)\n",
    "                    anchor_data_dic['review_comment_id'].append(review_comment.id)\n",
    "                    anchor_data_dic['created_at'].append(review_comment.created_at)\n",
    "                    anchor_data_dic['position'].append(review_comment.position)\n",
    "                    anchor_data_dic['diff_hunk'].append(review_comment.diff_hunk)\n",
    "                    anchor_data_dic['original_position'].append(review_comment.original_position)\n",
    "                    anchor_data_dic['commit_id'].append(review_comment.commit_id)\n",
    "                    anchor_data_dic['path'].append(review_comment.path)\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return anchor_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5464d2da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Reviewer simonbasle is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Page 60\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer twicksell is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer anuraaga is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Page 60\n",
      "Page 70\n",
      "Page 80\n",
      "Page 90\n",
      "Page 100\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer ivansenic is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer jeqo is being scraped #########\n",
      "Page 10\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer milgner is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer chao-chang-paypay is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer slomo is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer pirgeo is being scraped #########\n",
      "Page 10\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer lukaseder is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer spencergibb is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer kenfinnigan is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Page 60\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer breedx-nr is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer mheffner is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer tburch is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer sahilTakiar is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer eogren is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer zliu41 is being scraped #########\n",
      "Page 10\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer liyinan926 is being scraped #########\n",
      "Page 10\n",
      "Page 20\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer jinhyukchang is being scraped #########\n",
      "Page 10\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer lbendig is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer pcadabam-zz is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer ydai1124 is being scraped #########\n",
      "Converting the extracted data to .csv\n",
      "######### Reviewer htran1 is being scraped #########\n",
      "Page 10\n",
      "Converting the extracted data to .csv\n"
     ]
    }
   ],
   "source": [
    "if anchor_log_tracker['latest_reviewer'] != None:\n",
    "    # We had some runs and we should continue where we left off\n",
    "    start = reviewers.index(anchor_log_tracker['latest_reviewer'])\n",
    "        \n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "try:\n",
    "    for reviewed_by in reviewers[start:]:\n",
    "        anchor_log_tracker['latest_reviewer'] = reviewed_by\n",
    "\n",
    "        print('######### Reviewer {} is being scraped #########'.format(reviewed_by))\n",
    "        found = None\n",
    "        page_number = 1\n",
    "        URLs = []\n",
    "\n",
    "        while found != []:\n",
    "            \n",
    "            try:\n",
    "                # Slowing down the URL request rate to avoid 429 error !\n",
    "                time.sleep(10)\n",
    "                url = 'https://github.com/search?p={}&q=type%3Apr+reviewed-by%3A{}+comments%3A%3E{}&type=Issues'.format(page_number, reviewed_by)\n",
    "                r = requests.get(url)\n",
    "                r.raise_for_status()\n",
    "                page_source = r.text\n",
    "                found = re.findall(query1, page_source)\n",
    "\n",
    "                if page_number%10 == 0:\n",
    "                    print('Page {}'.format(page_number))\n",
    "\n",
    "                for i in range(0, len(found), 2):\n",
    "                    url = re.search(query2, found[i]).group(1)\n",
    "                    URLs.append(url)\n",
    "\n",
    "                page_number += 1\n",
    "            \n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # After gathering all reviews of a reviewer, we extract our desirable data out of them\n",
    "        print('Converting the extracted data to .csv')\n",
    "        URLs.sort()\n",
    "        recent_repo = None\n",
    "\n",
    "        if anchor_log_tracker['latest_link'] != None:\n",
    "            # We had some runs and we should continue where we left off\n",
    "            start = URLs.index(anchor_log_tracker['latest_link'])\n",
    "\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        for url in URLs[start:]:\n",
    "            anchor_log_tracker['latest_link'] = url\n",
    "            search_rate_limit = client.get_rate_limit().core\n",
    "\n",
    "            if search_rate_limit.remaining < 50:\n",
    "                reset_timestamp = calendar.timegm(search_rate_limit.reset.timetuple())\n",
    "                sleep_time = reset_timestamp - calendar.timegm(time.gmtime()) + 10\n",
    "                print('Going to sleep untill {}'.format(search_rate_limit.reset))\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            repo_name = url.split('/')[0] + '/' + url.split('/')[1]\n",
    "            if recent_repo != repo_name:\n",
    "                recent_repo = repo_name\n",
    "                pr_num = int(url.split('/')[3])\n",
    "                repo = client.get_repo(repo_name)\n",
    "                pr = repo.get_pull(pr_num)\n",
    "                anchor_data_dic = import_reviewer_review_comments(reviewed_by, pr, repo, anchor_data_dic)\n",
    "\n",
    "            else:\n",
    "                pr_num = int(url.split('/')[3])\n",
    "                pr = repo.get_pull(pr_num)\n",
    "                anchor_data_dic = import_reviewer_review_comments(reviewed_by, pr, repo, anchor_data_dic)\n",
    "\n",
    "        anchor_log_tracker['latest_link'] = None\n",
    "\n",
    "except:\n",
    "    # Save our final results\n",
    "    tf = open(\"anchor_latest_logger.json\", \"w\")\n",
    "    json.dump(anchor_log_tracker, tf)\n",
    "    tf.close()\n",
    "\n",
    "    df = pd.DataFrame(anchor_data_dic)\n",
    "    df.to_csv('anchoring_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c62f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
